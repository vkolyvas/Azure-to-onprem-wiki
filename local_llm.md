# Local LLM Stack (Ollama or LM Studio)
Local large language model stack for on‑prem inference.\

**Installation**
- Install Ollama or LM Studio on a supported host.

- Download models (Llama, Mistral, Gemma, etc.) via the tool’s interface or command line.

**Configuration**
- Enable the REST API or HTTP interface for programmatic access.

- Configure a system service (systemd, Windows service, etc.) to run at boot.

- Preload or auto‑load required models on startup for lower latency.
